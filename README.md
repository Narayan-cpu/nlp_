ğŸ§ âœ¨ Natural Language Processing (NLP) Overview
This project leverages key NLP techniques to process and analyze text data. Here are the core NLP concepts and preprocessing steps:

1ï¸âƒ£ Tokenization âœ‚ï¸
Splits raw text into smaller units called tokens (words, sentences, or subwords).
Transforms text into a format suitable for analysis or machine learning.

Example:
Input: "NLP is fun!"
Output tokens: ["NLP", "is", "fun", "!"]
2ï¸âƒ£ Lemmatization ğŸŒ±
Reduces words to their base or dictionary form (lemma), so different forms of a word are treated as one.

Example:
Input: "running", "ran", "runs"
Output lemma: "run"
3ï¸âƒ£ Stop Words Removal ğŸš«ğŸ—£ï¸
Removes common words (like "the", "is", "and") that may not carry useful info.
Helps focus on more meaningful words.

Example:
Input: "This is a sample sentence."
After removal: ["sample", "sentence"]
4ï¸âƒ£ One-Hot Encoding (OHE) ğŸ”¢
Represents words as binary vectors.
Each word has a unique spot in the vector.

Example:
Vocabulary: ["cat", "dog", "fish"]
"dog" â†’ [0, 1, 0]
5ï¸âƒ£ Word Embeddings (WE) ğŸ§©
Dense vector representations of words that capture their meanings.
Similar words are close together in the vector space.

Example:
"king" ğŸ‘‘ and "queen" ğŸ‘¸ have similar vectors, reflecting their semantic similarity.
6ï¸âƒ£ Word2Vec (W@V) ğŸ¤–
Popular technique to create word embeddings using shallow neural networks.

How it works:

CBOW: Predicts a word from its context.
Skip-Gram: Predicts context words from a given word.
Example:
Training on "The cat sits on the mat" helps Word2Vec learn that "cat" and "mat" often share contexts and should have similar vector representations.

ğŸš€ Explore & Experiment!
All examples and live models are right here in the repo!
Dive in, experiment, and have fun with NLP! ğŸ§‘â€ğŸ’»ğŸ“šâœ¨
